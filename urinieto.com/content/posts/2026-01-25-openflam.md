---
title: "OpenFLAM"
author: uri
type: post
date: 2026-01-25T11:41:48-08:00
url: openflam
draft: false
thumbnail:
  src: "/wp-content/uploads/2026/OpenFLAM.png"
  alt: OpenFLAM - Framewise Language-Audio Model
categories:
  - icml
  - opensource

---

FLAM is now Open Source!

We're releasing [OpenFLAM](https://github.com/adobe-research/openflam), the companion code to our ICML 2025 paper [Frame-wise Language-Audio Modeling](https://arxiv.org/abs/2505.05335).

{{< img src="/wp-content/uploads/2026/OpenFLAM.png" alt="" width="600" >}}

So what does FLAM actually do?

- **Zero-shot Sound Event Detection**: describe any sound in plain text, and FLAM will tell you _when_ it happens in your audio
- **Text-based Audio Retrieval**: search massive audio libraries using natural language queries

The key insight: while most audio-language models only give you clip-level understanding, FLAM localizes events at the frame level. Ask "where's the dog barking?" and get precise timestamps, not just "yes, there's a dog somewhere in this 10-minute file."

Additionally, it's highly efficient, light-weight... and on PyPi (`pip install openflam`) and HuggingFace! ğŸª¶ğŸ¤—

Huge thanks to my co-authors Yusong Wu, Christos Tsirigotis, Ke Chen, Anna Huang, Aaron Courville, Prem Seetharaman, and Justin Salamon for making this happen.

Here the links:
- ğŸ’» Code: https://github.com/adobe-research/openflam
- ğŸ“„ Paper: https://arxiv.org/abs/2505.05335 
- ğŸŒ Project page: https://flam-model.github.io/
- ğŸ¤— HuggingFace: https://huggingface.co/kechenadobe/OpenFLAM/tree/main

